{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UkouS8Y-o3R9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.19.1-cp312-cp312-win_amd64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.4.1-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\bonvi\\newanaconda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bonvi\\newanaconda\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\bonvi\\newanaconda\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\bonvi\\newanaconda\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bonvi\\newanaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bonvi\\newanaconda\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bonvi\\newanaconda\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\bonvi\\newanaconda\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\bonvi\\newanaconda\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bonvi\\newanaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bonvi\\newanaconda\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.4.1-cp312-cp312-win_amd64.whl (199.4 MB)\n",
      "   ---------------------------------------- 0.0/199.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 3.4/199.4 MB 22.5 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 8.7/199.4 MB 23.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 14.4/199.4 MB 25.2 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 20.2/199.4 MB 25.5 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 29.9/199.4 MB 29.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 39.3/199.4 MB 32.5 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 49.0/199.4 MB 34.3 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 58.7/199.4 MB 35.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 66.8/199.4 MB 36.1 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 76.3/199.4 MB 36.9 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 85.7/199.4 MB 37.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 95.4/199.4 MB 38.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 104.9/199.4 MB 38.7 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 114.0/199.4 MB 39.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 123.2/199.4 MB 39.5 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 132.6/199.4 MB 39.8 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 141.8/199.4 MB 40.1 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 150.7/199.4 MB 40.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 159.6/199.4 MB 40.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 169.3/199.4 MB 40.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 178.3/199.4 MB 40.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 187.7/199.4 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  196.3/199.4 MB 41.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  199.2/199.4 MB 41.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 199.4/199.4 MB 38.9 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.19.1-cp312-cp312-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 32.1 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.4.1-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 45.7 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.4.1 torchaudio-2.4.1 torchvision-0.19.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "70-XsHkDGUKu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 9359086306628950164\n",
       " xla_global_id: -1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if a GPU is available for training a model\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gZkz29ilGWd9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Display detailed information about the system's memory\n",
    "!cat /proc/meminfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6UbfwSrGZsH"
   },
   "outputs": [],
   "source": [
    "# Display detailed information about the system's central processing unit (CPU)\n",
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BREirreEUnGg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle==1.5.8\n",
      "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73262 sha256=537694cad7a5f2d4070148de6cdc5607fb742e31cbdfdc0d0120d57e8d27b942\n",
      "  Stored in directory: c:\\users\\bonvi\\appdata\\local\\pip\\cache\\wheels\\6f\\90\\b4\\bad113af4a919722b50a431481ea605c074772aab4d7d6072b\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle\n",
      "Successfully installed kaggle-1.5.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/.kaggle/kaggle.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmkdir /root/.kaggle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/root/.kaggle/kaggle.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkhushalidaga08\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc7aaa2a8acee7e33b56fe4195f198b4d\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Put your kaggle username & key here\u001b[39;00m\n",
      "File \u001b[1;32m~\\newAnaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.kaggle/kaggle.json'"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
    "!mkdir /root/.kaggle\n",
    "\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "    f.write('{\"username\":\"khushalidaga08\",\"key\":\"c7aaa2a8acee7e33b56fe4195f198b4d\"}')\n",
    "    # Put your kaggle username & key here\n",
    "\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "smnVE5DpUoxh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\bonvi\\newAnaconda\\Scripts\\kaggle.exe\\__main__.py\", line 4, in <module>\n",
      "  File \"C:\\Users\\bonvi\\newAnaconda\\Lib\\site-packages\\kaggle\\__init__.py\", line 23, in <module>\n",
      "    api.authenticate()\n",
      "  File \"C:\\Users\\bonvi\\newAnaconda\\Lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py\", line 164, in authenticate\n",
      "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
      "OSError: Could not find kaggle.json. Make sure it's located in C:\\Users\\bonvi\\.kaggle. Or use the environment method.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download behrad3d/nasa-cmaps\n",
    "! mkdir train\n",
    "! unzip nasa-cmaps.zip -d train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CzoL9ofEgJJf"
   },
   "outputs": [],
   "source": [
    "PM_train = '/content/train/CMaps/train_FD001.txt'\n",
    "PM_test = '/content/train/CMaps/test_FD001.txt'\n",
    "PM_truth = '/content/train/CMaps/RUL_FD001.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BsYPkY7gYGx"
   },
   "source": [
    "# Binary classification\n",
    "Predict if an asset will fail within certain time frame (e.g. cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QfpzPSgG-If3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from torchinfo import summary\n",
    "import keras\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'binary_model.keras'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EilFg--x-ety"
   },
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjbfnUZGgc3C"
   },
   "outputs": [],
   "source": [
    "# Read training data - Aircraft engine run-to-failure data\n",
    "train_df = NotImplemented # Read the txt file, use appropriate separator and header\n",
    "train_df.drop(NotImplemented, axis=1, inplace=True)  # Explore the data on your own and remove unnecessary columns\n",
    "train_df.columns = [NotImplemented]  # Assign names to all the columns\n",
    "\n",
    "train_df = train_df.sort_values([NotImplemented, NotImplemented])  # Sort by id and cycle\n",
    "\n",
    "# Read test data - Aircraft engine operating data without failure events recorded\n",
    "test_df = NotImplemented  # Read the txt file, use appropriate separator and header\n",
    "test_df.drop(NotImplemented, axis=1, inplace=True)  # Explore the data on your own and remove unnecessary columns\n",
    "test_df.columns = [NotImplemented]  # Assign names to all the columns\n",
    "\n",
    "# Read ground truth data - True remaining cycles for each engine in testing data\n",
    "truth_df = NotImplemented # Read the txt file, use appropriate separator and header\n",
    "truth_df.drop(NotImplemented, axis=1, inplace=True)  # Explore the data on your own and remove unnecessary columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEpD7amS-lpu"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulY14O06knOI"
   },
   "outputs": [],
   "source": [
    "#######\n",
    "# TRAIN\n",
    "#######\n",
    "# Data Labeling - generate column RUL (Remaining Useful Life or Time to Failure)\n",
    "\n",
    "# TODO: Calculate the maximum cycle value for each engine (id) and store it in a new DataFrame (rul)\n",
    "rul = NotImplemented\n",
    "# TODO: Rename the columns in the rul DataFrame\n",
    "rul.columns = NotImplemented\n",
    "# TODO: Merge the rul DataFrame with the original train_df based on the 'id' column\n",
    "train_df = NotImplemented\n",
    "# TODO: Calculate the Remaining Useful Life (RUL) by subtracting the current cycle from the maximum cycle\n",
    "train_df['RUL'] = NotImplemented\n",
    "# TODO: Remove the temporary column used to calculate RUL\n",
    "train_df.drop(NotImplemented, axis=1, inplace=True)\n",
    "\n",
    "# Generate label columns for training data\n",
    "# We will only make use of \"label1\" for binary classification,\n",
    "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
    "w1 = 30\n",
    "w0 = 15\n",
    "\n",
    "# TODO: Create a binary label ('label1') indicating if the engine will fail within w1 cycles (1) or not (0)\n",
    "train_df['label1'] = NotImplemented  # Replace with the correct threshold value and label values\n",
    "# TODO: Initialize a second label ('label2') as a copy of 'label1'\n",
    "train_df['label2'] = NotImplemented\n",
    "# TODO: Update 'label2' to indicate if the engine will fail within w0 cycles (2) or not (0/1)\n",
    "train_df.loc[NotImplemented] = NotImplemented # Replace with the correct threshold value and label value\n",
    "\n",
    "\n",
    "# MinMax normalization (from 0 to 1)\n",
    "# TODO: Create a normalized version of the 'cycle' column (e.g., 'cycle_norm') using the original 'cycle' values\n",
    "train_df['cycle_norm'] = NotImplemented  # Replace with the correct normalization code\n",
    "# TODO: Select the columns to be normalized (all columns except 'id', 'cycle', 'RUL', 'label1', and 'label2')\n",
    "cols_normalize = NotImplemented  # Replace with the correct column selection code\n",
    "# TODO: Initialize a MinMaxScaler object to scale values between 0 and 1\n",
    "min_max_scaler = NotImplemented  # Replace with the correct scaler initialization code\n",
    "# TODO: Apply MinMaxScaler to the selected columns and create a new normalized DataFrame\n",
    "norm_train_df = NotImplemented  # Replace with the correct normalization code\n",
    "# TODO: Join the normalized DataFrame with the original DataFrame (excluding normalized columns)\n",
    "join_df = NotImplemented  # Replace with the correct join code\n",
    "# TODO: Reorder the columns in the joined DataFrame to match the original order\n",
    "train_df = NotImplemented  # Replace with the correct reindexing code\n",
    "\n",
    "######\n",
    "# TEST\n",
    "######\n",
    "# MinMax normalization (from 0 to 1)\n",
    "# TODO: Similar to the MinMax normalization done for Train, complete the code below.\n",
    "test_df['cycle_norm'] = NotImplemented\n",
    "norm_test_df = NotImplemented\n",
    "test_join_df = NotImplemented\n",
    "test_df = NotImplemented\n",
    "test_df = NotImplemented\n",
    "\n",
    "# We use the ground truth dataset to generate labels for the test data.\n",
    "# generate column max for test data\n",
    "# TODO: Calculate the maximum cycle value for each engine (id) in the test data and store it in a new DataFrame (rul)\n",
    "rul = NotImplemented\n",
    "# TODO: Rename the columns in the rul DataFrame\n",
    "rul.columns = NotImplemented\n",
    "# TODO: Merge the rul DataFrame with the original test_df based on the 'id' column\n",
    "truth_df.columns = ['more']\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "# TODO: Remove the temporary column used to calculate RUL\n",
    "truth_df.drop(NotImplemented, axis=1, inplace=True)\n",
    "\n",
    "# TODO: Merge the adjusted truth_df with the test_df to generate RUL values for test data\n",
    "test_df = NotImplemented\n",
    "# TODO: Calculate the Remaining Useful Life (RUL) by subtracting the current cycle from the maximum cycle\n",
    "test_df['RUL'] = NotImplemented\n",
    "# TODO: Remove the temporary column used to calculate RUL\n",
    "test_df.drop(NotImplemented, axis=1, inplace=True)\n",
    "\n",
    "# Generate binary label columns (label1 and label2) based on RUL values and thresholds w0 and w1\n",
    "# TODO: Similar to what you did in the train dataframe\n",
    "test_df['label1'] = NotImplemented\n",
    "test_df['label2'] = NotImplemented\n",
    "test_df.loc[NotImplemented] = NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57FSFDb4-r3d"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NToa3NezS-OX"
   },
   "outputs": [],
   "source": [
    "# TODO: Define window size and sequence length\n",
    "sequence_length = NotImplemented  # Replace with the desired sequence length\n",
    "\n",
    "# Function to reshape features into (samples, time steps, features)\n",
    "def generate_sequences(id_df, sequence_length, feature_columns):\n",
    "    \"\"\"Generate sequences from a dataframe for a given id.\n",
    "    Sequences that are under the sequence length will be considered.\n",
    "    We can also pad the sequences in order to use shorter ones.\"\"\"\n",
    "    data_matrix = id_df[feature_columns].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "\n",
    "    for start, end in zip(range(0, num_elements - sequence_length), range(sequence_length, num_elements)):\n",
    "        yield NotImplemented  # TODO: Replace with the correct code to yield sequences of feature values\n",
    "\n",
    "# TODO: Select feature columns for sequence generation (e.g., sensor readings, settings)\n",
    "sensor_columns = NotImplemented  # TODO: Replace with the correct list of sensor column names\n",
    "sequence_columns = NotImplemented  # TODO: Replace with the correct list of sequence column names (including settings and sensors)\n",
    "\n",
    "# TODO: Generate sequences for all engine ids in the training data\n",
    "sequence_generator = NotImplemented  # TODO: Replace with the correct code to generate sequences\n",
    "\n",
    "# TODO: Convert generated sequences to a numpy array for LSTM input\n",
    "sequence_array = NotImplemented  # TODO: Replace with the correct code to convert sequences to numpy array\n",
    "\n",
    "# TODO: Function to generate labels\n",
    "def generate_labels(id_df, sequence_length, label_column):\n",
    "    \"\"\"Generate labels for a given id.\"\"\"\n",
    "    data_matrix = id_df[label_column].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    return NotImplemented  # TODO: Replace with the correct code to generate labels\n",
    "\n",
    "# TODO: Generate labels for all engine ids in the training data\n",
    "label_generator = NotImplemented  # TODO: Replace with the correct code to generate labels for all engine ids\n",
    "label_array = NotImplemented  # TODO: Replace with the correct code to convert labels to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_VqVFhdq0tR"
   },
   "outputs": [],
   "source": [
    "# Define the number of features and output units\n",
    "nb_features = sequence_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# TODO: Add LSTM layers and Dropout layers to the model\n",
    "# Note: Limit the total number of model parameters to 10,000\n",
    "# Your code here:\n",
    "\n",
    "\n",
    "# Add a Dense output layer with sigmoid activation\n",
    "model.add(Dense(units=nb_out, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with binary crossentropy loss and Adam optimizer\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# TODO: Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JUi08eUonh7"
   },
   "outputs": [],
   "source": [
    "# TODO: Fit the network to the training data\n",
    "history = model.fit(\n",
    "    sequence_array,\n",
    "    label_array,\n",
    "    epochs=NotImplemented,  # TODO: Replace with the desired number of training epochs\n",
    "    batch_size=NotImplemented,  # TODO: Replace with the desired batch size\n",
    "    validation_split=NotImplemented,  # TODO: Replace with the desired validation split proportion\n",
    "    verbose=NotImplemented,  # TODO: Replace with the desired verbosity level\n",
    "    callbacks = [\n",
    "        # TODO: Early stopping callback to stop training when validation loss stops improving\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=NotImplemented,  # TODO: Replace with the minimum change in validation loss to qualify as improvement\n",
    "            patience=NotImplemented,  # TODO: Replace with the number of epochs to wait before stopping training\n",
    "            verbose=NotImplemented,  # TODO: Replace with the desired verbosity level\n",
    "            mode='min'\n",
    "        ),\n",
    "        # TODO: Model checkpoint callback to save the best model based on validation loss\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            NotImplemented,  # TODO: Replace with the file path to save the best model\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            mode='min',\n",
    "            verbose=NotImplemented  # TODO: Replace with the desired verbosity level\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWyZJ2mP-1pB"
   },
   "source": [
    "## Model Evaluation on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpVYSzXkmk5l"
   },
   "outputs": [],
   "source": [
    "# TODO: summarize history for Accuracy\n",
    "# TODO: Plot the training & validation accuracy over epochs and display the plot\n",
    "# TODO: Save the plot to a file\n",
    "\n",
    "\n",
    "# TODO: summarize history for Loss\n",
    "# TODO: Plot the training & validation loss over epochs and display the plot\n",
    "# TODO: Save the plot to a file\n",
    "\n",
    "# TODO: Use the evaluate method to calculate the accuracy of the model on the training data\n",
    "scores = NotImplemented  # TODO: Replace with the correct code to evaluate the model on the training data\n",
    "\n",
    "# Print the accuracy of the model on the training data\n",
    "\n",
    "# make predictions and compute confusion matrix\n",
    "# TODO: Use the predict method to make predictions on the training data\n",
    "# TODO: Convert the predicted probabilities to class labels (e.g., using a threshold of 0.5)\n",
    "y_pred = NotImplemented # TODO: Use predict and convert probabilities to class labels\n",
    "y_true = label_array\n",
    "\n",
    "# TODO: Create a Pandas DataFrame from the predicted labels and save it to a CSV file\n",
    "test_set = NotImplemented  # TODO: Replace with the correct code to create a DataFrame from the predicted labels\n",
    "\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "# TODO: Compute the confusion matrix using confusion_matrix from sklearn.metrics\n",
    "cm = NotImplemented  # TODO: Replace with the correct code to compute the confusion matrix\n",
    "print(cm)\n",
    "\n",
    "# TODO: Calculate the precision using precision_score and recall using recall_score from sklearn.metrics\n",
    "precision = NotImplemented  # TODO: Replace with the correct code to calculate precision\n",
    "recall = NotImplemented  # TODO: Replace with the correct code to calculate recall\n",
    "print( 'precision = ', precision, '\\n', 'recall = ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxvEuR4S-6VI"
   },
   "source": [
    "## Model Evaluation on Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yT96j4rkL0K"
   },
   "outputs": [],
   "source": [
    "# TODO: Pick the last sequence for each id in the test data\n",
    "seq_array_test_last = NotImplemented  # Replace with code to select last sequence for each id\n",
    "\n",
    "# TODO: Convert to numpy array and ensure float32 data type\n",
    "seq_array_test_last = NotImplemented\n",
    "\n",
    "# TODO: Pick the labels for the selected sequences\n",
    "y_mask = NotImplemented  # TODO: Replace with code to select labels for sequences with length >= sequence_length\n",
    "label_array_test_last = NotImplemented  # TODO: Replace with code to select labels for the selected sequences\n",
    "\n",
    "# Reshape and ensure float32 data type\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "\n",
    "# TODO: Load the saved model if it exists\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = NotImplemented  # TODO: Replace with code to load the saved model\n",
    "\n",
    "# TODO: Evaluate the model on the test data\n",
    "scores_test = NotImplemented\n",
    "print('Accuracy: {}'.format(scores_test[1]))\n",
    "\n",
    "# TODO: Make predictions and compute confusion matrix\n",
    "y_pred_test = NotImplemented  # TODO: Replace with code to make predictions and convert to class labels\n",
    "y_true_test = label_array_test_last\n",
    "\n",
    "# TODO: Create pandas dataframe of y_pred_test and save predictions to CSV file\n",
    "test_set = NotImplemented\n",
    "\n",
    "# TODO: Compute confusion matrix\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = NotImplemented  # TODO: Replace with code to compute confusion matrix\n",
    "print(cm)\n",
    "\n",
    "# TODO: Compute precision, recall, and F1-score\n",
    "precision_test = NotImplemented  # TODO: Replace with code to compute precision\n",
    "recall_test = NotImplemented  # TODO: Replace with code to compute recall\n",
    "f1_test = NotImplemented  # TODO: Replace with code to compute F1-score\n",
    "print('Precision: ', precision_test, '\\n', 'Recall: ', recall_test, '\\n', 'F1-score:', f1_test)\n",
    "\n",
    "# TODO: Plot predicted and actual data for visual verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy1uAcAJqFf6"
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([[scores_test[1],precision_test,recall_test,f1_test],\n",
    "                          [0.94, 0.952381, 0.8, 0.869565]],\n",
    "                         columns = ['Accuracy', 'Precision', 'Recall', 'F1-score'],\n",
    "                         index = ['LSTM',\n",
    "                                 'Template Best Model'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwPU_ybuqSl7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
